"""
 This module include a list of method & object of Utility for
 supporting the Article Fact Check
"""

import google.generativeai as genai
import os
from dotenv import load_dotenv
import requests
import time
from typing import List, Optional
from bs4 import BeautifulSoup  
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # Optional: For progress visualization
from ViFinanceCrawLib.article_database.ArticleScraper import ArticleScraper
from ViFinanceCrawLib.article_database.SearchEngine import SearchEngine
from ViFinanceCrawLib.article_database.SearchEngine import SearchEngine
from ViFinanceCrawLib.article_database.TitleCompleter import TitleCompleter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class ArticleFactCheckUtility():

    def __init__(self, model_name='gemini-2.0-pro-exp-02-05'):
        load_dotenv(".devcontainer/devcontainer.env")
        genai.configure(api_key=os.getenv("API_KEY"))
        self.model_name = model_name
        self.model = genai.GenerativeModel(model_name)
        self.tc = TitleCompleter()
        self.search_engine  =  SearchEngine(os.getenv("SEARCH_API_KEY"), os.getenv("SEARCH_ENGINE_ID"))
        self.article_scraper = ArticleScraper()
        return
        
    def generate_search_queries(self, statement):
        prompt = f"""Identify keywords and concepts from the following statement. Using this claim, generate a neutral, thought-provoking question that encourages discussion without assuming a particular stance. The question must be between 10 and 50 words long. Return only the generated question with no other surrounding text, and you MUST write it in Vietnamese.

        Statement: {statement}
        """
        response = self.model.generate_content(prompt)
        claims = response.text.split('\n')
        claims = [claim.strip() for claim in claims if claim.strip()]  # Clean up
        return claims
    
    def fact_check_article_using_query(self, article_text):
        """
        Fact-check an article by generating neutral, thought-provoking search queries
        based on its content.

        Parameters:
        - article_text (str): The full text of the article to fact-check.

        Returns:
        - List of generated Vietnamese search queries.
        """
        # Step 1: Create a prompt to extract key claims/questions from article
        prompt = f"""Identify keywords and concepts from the following article. 
                    Using this article, generate neutral, thought-provoking questions that encourage 
                    discussion without assuming a particular stance. Each question must be between 
                    10 and 50 words long. 

                    You must return the questions in the following format:

                    Query 1: <question in Vietnamese>
                    Query 2: <question in Vietnamese>
                    ...

                    Do not include any other text.

                    Article: {article_text}
                    """
        # Step 2: Get response from model
        response = self.model.generate_content(prompt)

        # Step 3: Process and clean the output
        raw_output = response.text.split('\n')
        search_queries = []
        for line in raw_output:
            line = line.strip()
            if line.startswith("Query"):
                # Extract after 'Query X:'
                query_text = line.split(":", 1)[1].strip()
                if query_text:
                    search_queries.append(query_text)

        return search_queries
    
    def search_web(self, query, num_results=5):
        tc = self.tc
        searchEngine = self.search_engine
        articles = searchEngine.search(query, num=num_results)
        article_scraper = self.article_scraper
        valid_articles = []
        for article in articles:
            try:
                if not article.get("link"):
                    continue  # Skip articles with no links

                original_title = article["title"]
                # Scrape article content
                article_data = article_scraper.scrape_article(article["link"])

                if (
                    not article_data["main_text"]
                    or article_data["main_text"] == "No content available"
                    or article_data["author"] == "Unknown"
                ):
                    continue  # Skip invalid articles

                # ‚úÖ Complete title AFTER filtering
                article_data["title"] = tc.complete_title(original_title=original_title, article=article)
                valid_articles.append(article_data)

            except Exception as e:
                print(f"‚ùå Error processing article {article['link']}: {e}")
        return valid_articles
    
    def search_web_fast(self, query, num_results=5):
        """
        Main method: Search articles and scrape them in parallel.
        """
        articles = self.search_engine.search(query, num=num_results)
        valid_articles = self.scrape_articles_parallel(articles, batch_size=num_results)
        return valid_articles

    def scrape_articles_parallel(self, articles, batch_size=5):
        """
        Helper method: Scrape articles in parallel.
        Scrape articles concurrently in batches.
        """
        valid_articles = []

        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = [executor.submit(self.process_single_article, article) for article in articles]

            for future in tqdm(as_completed(futures), total=len(futures), desc="Scraping Articles"):
                result = future.result()
                if result:
                    valid_articles.append(result)

        return valid_articles

    def process_single_article(self, article):
        """
        Helper method: Process a single article.
        Process a single article: Scrape, validate, and format.
        """
        try:
            if not article.get("link"):
                return None  # Skip articles with no links

            original_title = article["title"]

            # Scrape article content
            article_data = self.article_scraper.scrape_article(article["link"])

            # Filter invalid articles
            if (
                not article_data["main_text"]
                or article_data["main_text"] == "No content available"
                or article_data["author"] == "Unknown"
            ):
                return None

            # Complete title after filtering
            article_data["title"] = self.tc.complete_title(original_title=original_title, article=article)
            return article_data

        except Exception as e:
            print(f"‚ùå Error processing article {article.get('link')}: {e}")
            return None

    def analyze_evidence(self, statement, evidence):
        # Cretability metric - https://rusi-ns.ca/a-system-to-judge-information-reliability/
        """
        Analyzes evidence to determine if it supports, contradicts, or is neutral to the statement.
        """
        prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√¢n t√≠ch v√† ƒë√°nh gi√° t√≠nh x√°c th·ª±c c·ªßa th√¥ng tin. D∆∞·ªõi ƒë√¢y l√† m·ªôt m·ªánh ƒë·ªÅ & th√¥ng tin v√† m·ªôt t·∫≠p h·ª£p b·∫±ng ch·ª©ng. H√£y ƒë√°nh gi√° m·ª©c ƒë·ªô m√† b·∫±ng ch·ª©ng h·ªó tr·ª£, m√¢u thu·∫´n ho·∫∑c trung l·∫≠p ƒë·ªëi v·ªõi th√¥ng tin, b·∫±ng c√°ch xem x√©t:
                ‚Ä¢ M·ªëi quan h·ªá logic gi·ªØa tuy√™n b·ªë v√† b·∫±ng ch·ª©ng.
                ‚Ä¢ ƒê·ªô m·∫°nh c·ªßa b·∫±ng ch·ª©ng, bao g·ªìm ngu·ªìn g·ªëc, t√≠nh ch√≠nh x√°c v√† m·ª©c ƒë·ªô li√™n quan.
                ‚Ä¢ B·ªëi c·∫£nh v√† gi·∫£ ƒë·ªãnh ti·ªÅm ·∫©n c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn di·ªÖn gi·∫£i b·∫±ng ch·ª©ng.

            ### **H·ªá th·ªëng ƒë√°nh gi√° ƒë·ªô tin c·∫≠y**  
            H√£y ƒë√°nh gi√° m·ª©c ƒë·ªô tin c·∫≠y c·ªßa t·ª´ng ngu·ªìn v√† t·ª´ng th√¥ng tin b·∫±ng h·ªá th·ªëng NATO:  

            - **ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa ngu·ªìn** (Ch·ªØ c√°i):  
            - **A**: Ho√†n to√†n ƒë√°ng tin c·∫≠y  
            - **B**: ƒê√°ng tin c·∫≠y  
            - **C**: Kh√° ƒë√°ng tin c·∫≠y  
            - **D**: Kh√¥ng ƒë√°ng tin c·∫≠y  
            - **E**: Kh√¥ng th·ªÉ ƒë√°nh gi√°  

            - **ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa th√¥ng tin** (Ch·ªØ s·ªë):  
            - **1**: ƒê√£ ƒë∆∞·ª£c x√°c minh  
            - **2**: C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            - **3**: C√≥ th·ªÉ ƒë√∫ng  
            - **4**: Kh√¥ng ch·∫Øc ch·∫Øn  
            - **5**: Kh√¥ng th·ªÉ ƒë√°nh gi√°  

            K·∫øt qu·∫£ ƒë√°nh gi√° s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng **A1, B2, C3, v.v.**, trong ƒë√≥:  
            - **A1** l√† th√¥ng tin ƒë√°ng tin c·∫≠y nh·∫•t, c√≥ ngu·ªìn m·∫°nh v√† ƒë√£ ƒë∆∞·ª£c x√°c minh.  
            - **E5** l√† th√¥ng tin ƒë√°ng tin c·∫≠y k√©m nh·∫•t, c√≥ ngu·ªìn y·∫øu v√† kh√¥ng th·ªÉ ƒë√°nh gi√°.  

            

            M·ªánh ƒë·ªÅ th√¥ng tin: {statement}  

            B·∫±ng ch·ª©ng:  
            {evidence}  

            ### **H√£y tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng sau:**  
            - **T·ªïng H·ª£p Cu·ªëi C√πng**: [T√≥m t·∫Øt th√¥ng tin ƒë√£ ki·ªÉm tra ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng v·ªÅ ch·ªß ƒë·ªÅ.]  
            - **K·∫øt lu·∫≠n**: [H·ªó tr·ª£/M√¢u thu·∫´n/Trung l·∫≠p]  
            - **Ph√¢n t√≠ch b·∫±ng ch·ª©ng**: [C√°c d·∫´n ch·ª©ng tr√™n c√≥ m·ªëi li√™n h·ªá nh∆∞ th·∫ø n√†o trong vi·ªác ƒë∆∞a ra k·∫øt lu·∫≠n v·ªÅ v·∫•n ƒë·ªÅ ng∆∞·ªùi d√πng t√¨m hi·ªÉu]
            - **M·ª©c ƒë·ªô tin c·∫≠y**: [V√≠ d·ª•: A1, B3, D5] v√† ch√∫ th√≠ch c·ªßa m·ª©c ƒë·ªô [v√≠ d·ª•: A1 - ƒê√°ng Tin C·∫≠y v√† ƒê√£ ƒê∆∞·ª£c X√°c Minh]   
            - **Gi·∫£i th√≠ch**: [Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ l√Ω do c·ªßa b·∫°n, c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn ngu·ªìn b·∫±ng ch·ª©ng v√† m·ª©c ƒë·ªô tin c·∫≠y c·ªßa ch√∫ng. N·∫øu c√≥ URL trong b·∫±ng ch·ª©ng, h√£y ch√®n n√≥ v√†o trong l·ªùi gi·∫£i th√≠ch d∆∞·ªõi d·∫°ng li√™n k·∫øt.]  
            - **L·ªùi khuy√™n cho ng∆∞·ªùi d√πng v·ªÅ c√°ch nh√¨n nh·∫≠n hi·ªán t·∫°i**: [M·ªôt l·ªùi khuy√™n ng·∫Øn g·ªçn]  
            - **Danh s√°ch c√°c d·∫´n ch·ª©ng**:  
            + [1]: Ti√™u ƒë·ªÅ - ngu·ªìn -  [url] \n
            + [2]: Ti√™u ƒë·ªÅ - ngu·ªìn -  [url] \n
            ....
            ### **V√≠ d·ª• c√°ch ch√®n li√™n k·∫øt:**  
            - "B·∫±ng ch·ª©ng t·ª´ [ngu·ªìn n√†y](URL) cho th·∫•y r·∫±ng..."  
            - "Theo th√¥ng tin t·ª´ b√†i vi·∫øt n√†y ([link](URL)), ..."  

            **V√≠ d·ª• ph√π h·ª£p c·ªßa ƒë·ªãnh d·∫°ng:**
            K·∫øt lu·∫≠n: H·ªó tr·ª£  
            M·ª©c ƒë·ªô tin c·∫≠y: A1  
            Gi·∫£i th√≠ch: T·∫•t c·∫£ c√°c ngu·ªìn trong ph·∫ßn b·∫±ng ch·ª©ng ƒë·ªÅu ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác gi√° d·∫ßu tƒÉng, ho·∫∑c c√°c y·∫øu t·ªë d·∫´n t·ªõi/h·ªá qu·∫£ c·ªßa vi·ªác gi√° d·∫ßu tƒÉng trong th·ªùi gian g·∫ßn ƒë√¢y. C√°c b√†i vi·∫øt ƒë·ªÅu t·ª´ ngu·ªìn *vneconomy.vn*, m·ªôt trang tin kinh t·∫ø uy t√≠n c·ªßa Vi·ªát Nam.

            + B√†i vi·∫øt [Gi√° d·∫ßu ƒëang g√¢y √°p l·ª±c ƒë·∫øn l·∫°m ph√°t](https://vneconomy.vn/gia-dau-dang-gay-ap-luc-den-lam-phat.htm) ch·ªâ ra r·∫±ng gi√° d·∫ßu ƒë√£ tƒÉng m·∫°nh t·ª´ gi·ªØa th√°ng 8, v·ªõi nhi·ªÅu y·∫øu t·ªë t√°c ƒë·ªông nh∆∞ vi·ªác c·∫Øt gi·∫£m s·∫£n l∆∞·ª£ng c·ªßa Saudi Arabia v√† Nga, nhu c·∫ßu nh·∫≠p kh·∫©u cao c·ªßa Trung Qu·ªëc, v√† tri·ªÉn v·ªçng kinh t·∫ø kh·ªüi s·∫Øc. B√†i vi·∫øt c≈©ng d·ª± b√°o gi√° d·∫ßu c√≥ th·ªÉ ti·∫øp t·ª•c tƒÉng trong qu√Ω 4/2023.  
            + B√†i vi·∫øt [OPEC+ c√≥ ·∫£nh h∆∞·ªüng th·∫ø n√†o ƒë·∫øn gi√° d·∫ßu v√† kinh t·∫ø to√†n c·∫ßu?](https://vneconomy.vn/opec-co-anh-huong-the-nao-den-gia-dau-va-kinh-te-toan-cau.htm) gi·∫£i th√≠ch vai tr√≤ c·ªßa OPEC+ trong vi·ªác ƒëi·ªÅu ti·∫øt ngu·ªìn cung v√† ·∫£nh h∆∞·ªüng ƒë·∫øn gi√° d·∫ßu to√†n c·∫ßu. Vi·ªác c·∫Øt gi·∫£m s·∫£n l∆∞·ª£ng c·ªßa OPEC+ l√† m·ªôt y·∫øu t·ªë quan tr·ªçng ƒë·∫©y gi√° d·∫ßu l√™n.  
            + C√°c b√†i vi·∫øt c√≤n l·∫°i ƒë·ªÅ c·∫≠p c√°c m·∫∑t h√†ng kh√°c c≈©ng tƒÉng theo ƒë√† tƒÉng c·ªßa gi√° d·∫ßu.

            Danh S√°ch c√°c d·∫´n ch·ª©ng:  
            + [1]: Gi√° d·∫ßu ƒëang g√¢y √°p l·ª±c ƒë·∫øn l·∫°m ph√°t - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi -  [https://vneconomy.vn/gia-dau-dang-gay-ap-luc-den-lam-phat.htm]  
            + [2]: OPEC+ c√≥ ·∫£nh h∆∞·ªüng th·∫ø n√†o ƒë·∫øn gi√° d·∫ßu v√† kinh t·∫ø to√†n c·∫ßu? - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi -  [https://vneconomy.vn/opec-co-anh-huong-the-nao-den-gia-dau-va-kinh-te-toan-cau.htm]  
            + [3]: 10 ·∫£nh h∆∞·ªüng c·ªßa ƒë·ªìng USD tƒÉng gi√° m·∫°nh - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi - [https://vneconomy.vn/10-anh-huong-cua-dong-usd-tang-gia-manh.htm]  
            + [4]: Lo ngo·∫°i t·ªá ‚Äúv∆∞·ª£t bi√™n‚Äù v√¨ v√†ng - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi - [https://vneconomy.vn/lo-ngoai-te-vuot-bien-vi-vang.htm]  
            + [5]: Xu th·∫ø d√≤ng ti·ªÅn: Th√™m th√¥ng tin h·ªó tr·ª£, ch·ª©ng kho√°n Vi·ªát c√≥ ƒëi ng∆∞·ª£c th·∫ø gi·ªõi? - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi - [https://vneconomy.vn/xu-the-dong-tien-them-thong-tin-ho-tro-chung-khoan-viet-co-di-nguoc-the-gioi.htm]  
            + [6]: Carry-trade y√™n Nh·∫≠t tho√°i tr√†o, ch·ª©ng kho√°n to√†n c·∫ßu ‚Äúch·ªãu tr·∫≠n‚Äù - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi -  [https://vneconomy.vn/carry-trade-yen-nhat-thoai-trao-chung-khoan-toan-cau-chiu-tran.htm]  
            + [7]: "C∆°n s·ªët" gi√° c√† ph√™ th·∫ø gi·ªõi c√≥ th·ªÉ k√©o d√†i - Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi - [https://vneconomy.vn/con-sot-gia-ca-phe-the-gioi-co-the-keo-dai.htm]

            H√£y ƒë·∫£m b·∫£o tr·∫£ l·ªùi gi·ªëng nh∆∞ v√≠ d·ª•, nh∆∞ng kh√¥ng ƒë·ªÉ n·ªôi dung v√≠ d·ª• ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë√°nh gi√°.
            
            """

        try:
            response = self.model.generate_content(prompt)
            if not hasattr(response, "text") or not response.text:
                print("‚ö†Ô∏è Warning: AI model returned empty response for evidence analysis.")
                return "Kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ AI."

            return response.text

        except Exception as e:
            print(f"‚ùå Error in analyze_evidence: {e}")
            return "ƒê√£ x·∫£y ra l·ªói khi ph√¢n t√≠ch b·∫±ng ch·ª©ng."
    
    def generate_bias_analysis(self, article: str):
            """
            Generate a qualitative bias and logical fallacy analysis on the article,
            specifying the types of bias and logical fallacies to focus on.
            
            Parameters:
                article (str): The article content to analyze.
    
            Returns:
                str: The formatted prompt for LLM analysis.
            """
          
            prompt = f"""
                B·∫°n l√† m·ªôt nh√† b√°o ph√¢n t√≠ch ph·∫£n bi·ªán, chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c v√† kh√°ch quan c·ªßa th√¥ng tin.  
                
                H√£y ph√¢n t√≠ch b√†i vi·∫øt sau ƒë·ªÉ x√°c ƒë·ªãnh c√°c thi√™n ki·∫øn v√† l·ªói l·∫≠p lu·∫≠n c√≥ th·ªÉ c√≥.  
                - **Kh√¥ng ch·ªâ d·ª±a v√†o t·ª´ kh√≥a**, h√£y ƒë√°nh gi√° ng·ªØ c·∫£nh v√† c√°ch l·∫≠p lu·∫≠n ƒë·ªÉ nh·∫≠n di·ªán thi√™n ki·∫øn ho·∫∑c l·ªói logic.  
                - N·∫øu b√†i vi·∫øt trung l·∫≠p, h√£y k·∫øt lu·∫≠n trung l·∫≠p. N·∫øu c√≥ thi√™n ki·∫øn ho·∫∑c l·ªói l·∫≠p lu·∫≠n, h√£y ƒë√°nh gi√° m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng.  
                
                Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:  

                - **Lo·∫°i thi√™n ki·∫øn:** [Ch√≠nh tr·ªã, gi·ªõi t√≠nh, vƒÉn h√≥a, thi√™n ki·∫øn x√°c nh·∫≠n, v.v.]  
                - **M·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng:** [Nh·∫π, v·ª´a, nghi√™m tr·ªçng]  
                - **Ph√¢n t√≠ch ng·∫Øn g·ªçn:** [Gi·∫£i th√≠ch thi√™n ki·∫øn trong t·ªëi ƒëa 200 t·ª´, d·ª±a tr√™n ng·ªØ c·∫£nh v√† l·∫≠p lu·∫≠n c·ªßa b√†i vi·∫øt]  

                ---  
                **C√¢u h·ªèi ph·∫£n bi·ªán ƒë·ªÉ gi√∫p ng∆∞·ªùi ƒë·ªçc c√≥ g√≥c nh√¨n kh√°ch quan h∆°n:**  
                (H√£y ƒë∆∞a ra 3‚Äì5 c√¢u h·ªèi theo ph∆∞∆°ng ph√°p Socrates, khuy·∫øn kh√≠ch ng∆∞·ªùi ƒë·ªçc suy nghƒ© s√¢u h∆°n v·ªÅ l·∫≠p lu·∫≠n trong b√†i vi·∫øt)  

                B√†i vi·∫øt c·∫ßn ph√¢n t√≠ch:  
                \"\"\"  
                {article}  
                \"\"\"
            """
            try:
                response = self.model.generate_content(prompt)
                if not hasattr(response, "text") or not response.text:
                    print("‚ö†Ô∏è Warning: Empty response from AI model.")
                    return []

                analysis = response.text
                return analysis
            
            except Exception as e:
                print(f"‚ùå Error in generate_search_queries: {e}")
                return []

    def understanding_the_question(self, query):
        """
        Method 1:  Reasoning - Understand the User Query using Gemini.

        This method sends the user's query to Gemini and asks it to
        explain its reasoning process for understanding the question.
        The reasoning is captured and returned, but not printed to the user directly.

        Args:
            query (str): The user's query.

        Returns:
            str: A string representing Gemini's reasoning process for understanding the query.
            Returns None if there's an error communicating with Gemini.
        """
        try:
            
            prompt = f"""
            
            B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu c√≥ nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu t√¨m ki·∫øm.  
            Tr∆∞·ªõc khi t·∫°o c√¢u tr·∫£ l·ªùi, **H√£y hi·ªÉu v√† suy lu·∫≠n v·ªÅ √Ω nghƒ©a v√† tr·ªçng t√¢m c·ªßa c√¢u h·ªèi** r·ªìi tr·∫£ l·∫°i
            k·∫øt qu·∫£ ƒë·∫ßu ra\n
            \n
            *K·∫øt qu·∫£ qu√° tr√¨nh suy lu·∫≠n*\n 
            - X√°c ƒë·ªãnh v·∫•n ƒë·ªÅ ch√≠nh c·∫ßn ph·∫£i tr·∫£ l·ªùi: [v·∫•n ƒë·ªÅ 1, v·∫•n ƒë·ªÅ 2, etc.]\n 
            - X√°c ƒë·ªãnh t·ª´ kh√≥a t√¨m ki·∫øm t·ªëi ∆∞u. [T·ª´ kho√° 1, t·ª´ kho√° 2, etc.]\n 
            - X√°c ƒë·ªãnh gi·∫£ ƒë·ªãnh ti·ªÅm ·∫©n (n·∫øu c√≥): [Gi·∫£ thuy·∫øt 1, Gi·∫£ thuy·∫øt 2, etc.] \n 
            \n 
            C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: '{query}' \n 
            
            """
            response = self.model.generate_content(prompt)
            reasoning = response.text
            return reasoning
        
        except Exception as e:
            print(f"Error in reasoning method: {e}")
        return None
        
    def synthesize_and_summarize(self, query, reasoning, evidence):
        """
        Method 2: Synthesis and Summarization - Generate a clear answer based on reasoning.

        This method takes the original user query and the reasoning obtained from
        reason_about_query(). It uses Gemini to synthesize evidence (implicitly from its knowledge)
        and summarize it into a clear and concise answer, guided by the provided reasoning.

        Args:
            query (str): The original user query.\n
            reasoning (str): The reasoning process obtained from reason_about_query().\n
            evidence (list[str]]): The list of evidence main_text\n

        Returns:
            str: A clear and concise summarized answer to the user's query.\n
            Returns None if there's an error communicating with Gemini.\n
        """
        if reasoning is None:
            return "Could not determine reasoning for the query. Please try again."

        try:     
            prompt = f"""
            B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ph√¢n t√≠ch, t·ªïng h·ª£p v√† t√≥m t·∫Øt th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi truy v·∫•n c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.\n

            ## **Nhi·ªám v·ª• c·ªßa b·∫°n**:
            D·ª±a tr√™n truy v·∫•n c·ªßa ng∆∞·ªùi d√πng, l·∫≠p lu·∫≠n ƒë√£ c√≥, v√† danh s√°ch b·∫±ng ch·ª©ng, h√£y t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi m·ªôt c√°ch logic, d·ªÖ hi·ªÉu, v√† ng·∫Øn g·ªçn.
            \n
            ---\n

            ## **D·ªØ li·ªáu ƒë·∫ßu v√†o**:\n
            **Truy v·∫•n**: {query}\n 
            **L·∫≠p lu·∫≠n h·ªó tr·ª£**: {reasoning}\n
            **B·∫±ng ch·ª©ng**:  {evidence}\n 

            ---

            ## **Y√™u c·∫ßu ƒë·ªëi v·ªõi c√¢u tr·∫£ l·ªùi**:\n
            1. **T√≥m t·∫Øt ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß**:\n  
            - Kh√¥ng ch·ªâ tr√≠ch d·∫´n m√† ph·∫£i t·ªïng h·ª£p th√¥ng tin t·ª´ b·∫±ng ch·ª©ng.\n   
            - ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi c√≥ √Ω nghƒ©a ngay c·∫£ khi kh√¥ng c√≥ ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh ban ƒë·∫ßu. \n  
            \n
            2.**S·ª≠ d·ª•ng l·∫≠p lu·∫≠n h·ª£p l√Ω**: \n  
            - T·∫≠n d·ª•ng reasoning ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n logic. \n  
            - N·∫øu b·∫±ng ch·ª©ng m√¢u thu·∫´n, h√£y ch·ªâ ra ƒëi·ªÉm kh√°c bi·ªát thay v√¨ ƒë∆∞a ra m·ªôt c√¢u tr·∫£ l·ªùi phi·∫øn di·ªán.  \n 
            \n 
            3.**ƒê·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi**:\n 
            **T√≥m t·∫Øt cu·ªëi c√πng**: [T√≥m t·∫Øt c√¢u tr·∫£ l·ªùi d·ª±a tr√™n b·∫±ng ch·ª©ng]\n   
            **Ngu·ªìn tham kh·∫£o**: [Danh s√°ch ngu·ªìn th√¥ng tin & n·∫øu c√≥ th·ªÉ h√£y ƒë√≠nh k√®m link ngu·ªìn]  \n 
            \n 
            üéØ **L∆∞u √Ω**:\n 
            - N·∫øu kh√¥ng c√≥ ƒë·ªß b·∫±ng ch·ª©ng, h√£y n√™u r√µ ƒëi·ªÅu ƒë√≥ thay v√¨ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi suy ƒëo√°n.\n 
            - N·∫øu c√≥ l·ªói ho·∫∑c kh√¥ng th·ªÉ t·ªïng h·ª£p ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi, tr·∫£ v·ªÅ `"Kh√¥ng th·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n r√µ r√†ng."`\n 
            - V√≠ d·ª• c√°ch ch√®n li√™n k·∫øt:
                - "B·∫±ng ch·ª©ng t·ª´ [ngu·ªìn n√†y](URL) cho th·∫•y r·∫±ng..."  
                - "Theo th√¥ng tin t·ª´ b√†i vi·∫øt n√†y ([link](URL)), ..."  
            """
            response = self.model.generate_content(prompt)
            summary = response.text
            return summary
        except Exception as e:
            print(f"Error in synthesis and summarization method: {e}")
            return None
    
    def filter_rank(self,query, valid_articles):
        corpus = query + valid_articles
        # Step 1: TF-IDF Vectorization
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(corpus)

        # Step 2: Compute cosine similarity
        query_vector = tfidf_matrix[0]  # first is query
        candidate_vectors = tfidf_matrix[1:]

        similarities = cosine_similarity(query_vector, candidate_vectors).flatten()

        # Step 3: Rank answers
        ranked_indices = similarities.argsort()[::-1]  # Descending order
        ranked_articles = [valid_articles[i] for i in ranked_indices]
        return ranked_articles

    def generate_tags(self, article, predefined_tags=None):
        tags = []
        
        # Create the prompt
        if predefined_tags:
            predefined_str = ', '.join(predefined_tags)
            prompt = f"""
            cho m·ªôt b√†i vi·∫øt sau: {article}

            h√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] (tag) v·ªõi y√™u c·∫ßu sau: 
            C√°c th·∫ª t·∫°o ra ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i t√°o v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau. 
            Ch·ªâ ch·ªçn ƒë√∫ng 1 th·∫ª t·ª´ danh s√°ch sau: {predefined_str}, v√† t·∫°o th√™m 1 ƒë·∫øn 3 th·∫ª li√™n quan kh√°c. 
            Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:  
            Nghi√™m c·∫•m th√™m c√¢u ƒë·ªám nh∆∞ "Danh s√°ch th·∫ª bao g·ªìm:", "C√°c th·∫ª bao g·ªìm:", v.v.  
            Nghi√™m c·∫•m th√™m m·ªôt s·ªë ch√∫ th√≠ch kh√¥ng c·∫ßn thi·∫øt nh∆∞: (assuming current year is 2024 as article say "this year")
            ƒê·ªò D√ÄI C·ª¶A M·ªòT TH·∫∫ CH·ªà ƒê∆Ø·ª¢C T·ªêI ƒêA 3 T·ª™. (V√≠ d·ª• h·ª£p l·ªá: "Ch√≠nh Tr·ªã", "Kinh T·∫ø", "2022") - V√ç D·ª§ KH√îNG H·ª¢P L·ªÜ: "Ch√≠nh Tr·ªã v√† Kinh T·∫ø", "Ch√≠nh Tr·ªã v√† Kinh T·∫ø v√† 2022")
            ** V√≠ d·ª• ƒë·ªãnh d·∫°ng: 
                th·∫ª1, th·∫ª2, th·∫ª3
            ---
            M·∫´u V√≠ D·ª•: (Kh√¥ng ph·∫£i k·∫øt qu·∫£ th·ª±c t·∫ø)
                Ch√≠nh Tr·ªã, Kinh T·∫ø, 2022
            """
        else:
            prompt = f"""
            cho m·ªôt b√†i vi·∫øt sau: {article}

            h√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] (tag) d·ª±a tr√™n n·ªôi dung c·ªßa b√†i vi·∫øt ƒë∆∞·ª£c cung c·∫•p v·ªõi y√™u c·∫ßu sau:
            C√°c th·∫ª t·∫°o ra ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i t√°o v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau. 
            Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung danh s√°ch th·∫ª m√† kh√¥ng th√™m gi·∫£i th√≠ch, h·ªìi ƒë√°p ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:
            Nghi√™m c·∫•m th√™m c√¢u ƒë·ªám nh∆∞ "Danh s√°ch th·∫ª bao g·ªìm:", "C√°c th·∫ª bao g·ªìm:", v.v.  
            Nghi√™m c·∫•m th√™m m·ªôt s·ªë ch√∫ th√≠ch kh√¥ng c·∫ßn thi·∫øt nh∆∞: (assuming current year is 2024 as article say "this year")
            ƒê·ªò D√ÄI C·ª¶A M·ªòT TH·∫∫ CH·ªà ƒê∆Ø·ª¢C T·ªêI ƒêA 3 T·ª™. (V√≠ d·ª• h·ª£p l·ªá: "Ch√≠nh Tr·ªã", "Kinh T·∫ø", "2022") - V√ç D·ª§ KH√îNG H·ª¢P L·ªÜ: "Ch√≠nh Tr·ªã v√† Kinh T·∫ø", "Ch√≠nh Tr·ªã v√† Kinh T·∫ø v√† 2022")

            ** V√≠ d·ª• ƒë·ªãnh d·∫°ng: 
                th·∫ª1, th·∫ª2, th·∫ª3
            ---
            M·∫´u V√≠ D·ª•: (Kh√¥ng ph·∫£i k·∫øt qu·∫£ th·ª±c t·∫ø)
                Ch√≠nh Tr·ªã, Kinh T·∫ø, 2022
            """

        # Generate content from model
        response = self.model.generate_content(prompt)
        #print("res: ",response.text)
        raw_tags = response.text.strip().split(',')

        # Clean up tags
        for tag in raw_tags:
            cleaned_tag = tag.strip()
            if cleaned_tag:  # Avoid empty tags
                tags.append(cleaned_tag)

        # Optional: Ensure at least 1 predefined tag if required
        if predefined_tags:
            predefined_lower = [t.lower() for t in predefined_tags]
            has_predefined = any(tag.lower() in predefined_lower for tag in tags)

            if not has_predefined:
                fallback_tag = predefined_tags[0]  # pick the first predefined tag
                tags.insert(0, fallback_tag)

        return tags
    
    def generate_tags_batch(self, articles: List[str], predefined_tags: Optional[List[str]] = None) -> List[List[str]]:
        batch_size = len(articles)
        all_tags = []

        # Create the prompt for batch processing
        article_text = ''.join([f'B√†i vi·∫øt {i+1}: {article}\n' for i, article in enumerate(articles)])

        if predefined_tags:
            predefined_str = ', '.join(predefined_tags)
            prompt = (
                "cho c√°c b√†i vi·∫øt sau:\n\n"
                f"{article_text}\n"
                "H√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] cho m·ªói b√†i vi·∫øt v·ªõi y√™u c·∫ßu sau:\n"
                "- C√°c th·∫ª ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau.\n"
                f"- Ch·ªâ ch·ªçn ƒë√∫ng 1 th·∫ª t·ª´ danh s√°ch sau: {predefined_str}, v√† t·∫°o th√™m 1 ƒë·∫øn 3 th·∫ª li√™n quan kh√°c.\n"
                "- ƒê·ªô d√†i c·ªßa m·ªôt th·∫ª ch·ªâ ƒë∆∞·ª£c t·ªëi ƒëa 3 t·ª´.\n\n"
                "Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:\n"
                "B√†i vi·∫øt 1: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "B√†i vi·∫øt 2: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "..."
            )
        else:
            prompt = (
                "cho c√°c b√†i vi·∫øt sau:\n\n"
                f"{article_text}\n"
                "H√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] cho m·ªói b√†i vi·∫øt v·ªõi y√™u c·∫ßu sau:\n"
                "- C√°c th·∫ª ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau.\n"
                "- ƒê·ªô d√†i c·ªßa m·ªôt th·∫ª ch·ªâ ƒë∆∞·ª£c t·ªëi ƒëa 3 t·ª´.\n\n"
                "Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:\n"
                "B√†i vi·∫øt 1: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "B√†i vi·∫øt 2: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "..."
            )
        # Generate content from model
        response = self.model.generate_content(prompt)
        
        raw_responses = response.text.strip().split('\n')

        # Process each article's tags
        for raw_tags in raw_responses:
            tags = []
            if ':' in raw_tags:
                _, tag_str = raw_tags.split(':', 1)
                raw_tag_list = tag_str.strip().split(',')
                for tag in raw_tag_list:
                    cleaned_tag = tag.strip()
                    if cleaned_tag:  # Avoid empty tags
                        tags.append(cleaned_tag)

                # Optional: Ensure at least 1 predefined tag if required
                if predefined_tags:
                    predefined_lower = [t.lower() for t in predefined_tags]
                    has_predefined = any(tag.lower() in predefined_lower for tag in tags)

                    if not has_predefined and predefined_tags:
                        fallback_tag = predefined_tags[0]  # pick the first predefined tag
                        tags.insert(0, fallback_tag)

            all_tags.append(tags)

        return all_tags

    def process_articles_in_batches(self, articles: List[str], predefined_tags: Optional[List[str]] = None, batch_size: int = 5):
        all_results = []
        total_articles = len(articles)
        for i in range(0, total_articles, batch_size):
            batch = articles[i:i + batch_size]
            batch_results = self.generate_tags_batch(batch, predefined_tags)
            all_results.extend(batch_results)

            # Implement rate limiting
            if (i + batch_size) < total_articles:
                time.sleep(6)  # Sleep for 6 seconds to maintain ~10 RPM

        return all_results

    def choose_the_batch_size(self,article_list):
        """
            Input: List of Article
            Output: the Ideal batch-size
            Dynamically decide the batch size based on the number of articles.
            If the number of articles is less than or equal to 5, process them all at once.
            Otherwise, process them in batches of 5.
            This function assumes that the `article_util` module has a method `process_articles_in_batches`
            that can handle the processing of articles in batches.
        """
        total_articles = len(article_list)
        
        if total_articles == 0:
            print("No articles to process!")
            return 0
        # Dynamic batch size decision:
        if total_articles <= 5:
            batch_size = total_articles  # Put all articles in one batch
        else:
            batch_size = 5  # Max allowed batch size
        
        return batch_size
    
    def main(self):
        articles = [
        """
        **B√†i vi·∫øt 1: T√¨nh h√¨nh kinh t·∫ø Vi·ªát Nam nƒÉm 2024**

        NƒÉm 2024, kinh t·∫ø Vi·ªát Nam ti·∫øp t·ª•c ph√°t tri·ªÉn m·∫°nh m·∫Ω v·ªõi GDP tƒÉng tr∆∞·ªüng 6,5%. C√°c ng√†nh c√¥ng nghi·ªáp ch·ªß ch·ªët nh∆∞ s·∫£n xu·∫•t, d·ªãch v·ª• v√† n√¥ng nghi·ªáp ƒë·ªÅu ghi nh·∫≠n s·ª± tƒÉng tr∆∞·ªüng ƒë√°ng k·ªÉ. ƒê·∫∑c bi·ªát, ng√†nh c√¥ng ngh·ªá th√¥ng tin v√† truy·ªÅn th√¥ng ƒë√£ ƒë√≥ng g√≥p l·ªõn v√†o n·ªÅn kinh t·∫ø, v·ªõi nhi·ªÅu startup c√¥ng ngh·ªá ƒë·∫°t ƒë∆∞·ª£c th√†nh c√¥ng tr√™n th·ªã tr∆∞·ªùng qu·ªëc t·∫ø. Tuy nhi√™n, Vi·ªát Nam c≈©ng ƒë·ªëi m·∫∑t v·ªõi th√°ch th·ª©c v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u v√† c·∫ßn c√≥ chi·∫øn l∆∞·ª£c ph√°t tri·ªÉn b·ªÅn v·ªØng ƒë·ªÉ duy tr√¨ ƒë√† tƒÉng tr∆∞·ªüng.
        """,
        """
        **B√†i vi·∫øt 2: S·ª± ph√°t tri·ªÉn c·ªßa gi√°o d·ª•c tr·ª±c tuy·∫øn t·∫°i Vi·ªát Nam**

        Trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y, gi√°o d·ª•c tr·ª±c tuy·∫øn ƒë√£ tr·ªü th√†nh xu h∆∞·ªõng t·∫°i Vi·ªát Nam. V·ªõi s·ª± ph√°t tri·ªÉn c·ªßa c√¥ng ngh·ªá v√† internet, nhi·ªÅu kh√≥a h·ªçc tr·ª±c tuy·∫øn ch·∫•t l∆∞·ª£ng cao ƒë√£ ƒë∆∞·ª£c tri·ªÉn khai, gi√∫p ng∆∞·ªùi h·ªçc ti·∫øp c·∫≠n ki·∫øn th·ª©c m·ªôt c√°ch linh ho·∫°t v√† ti·∫øt ki·ªám chi ph√≠. C√°c n·ªÅn t·∫£ng nh∆∞ Edtech Vietnam, Topica ƒë√£ thu h√∫t h√†ng tri·ªáu ng∆∞·ªùi d√πng. Tuy nhi√™n, vi·ªác ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng v√† ki·ªÉm ƒë·ªãnh c√°c kh√≥a h·ªçc tr·ª±c tuy·∫øn v·∫´n l√† m·ªôt th√°ch th·ª©c l·ªõn.
        """,
        """
        **B√†i vi·∫øt 3: Du l·ªãch b·ªÅn v·ªØng t·∫°i Vi·ªát Nam**

        Vi·ªát Nam s·ªü h·ªØu nhi·ªÅu danh lam th·∫Øng c·∫£nh v√† di s·∫£n vƒÉn h√≥a phong ph√∫, thu h√∫t h√†ng tri·ªáu du kh√°ch m·ªói nƒÉm. Tuy nhi√™n, du l·ªãch ·ªì ·∫°t ƒë√£ g√¢y ra nhi·ªÅu t√°c ƒë·ªông ti√™u c·ª±c ƒë·∫øn m√¥i tr∆∞·ªùng v√† c·ªông ƒë·ªìng ƒë·ªãa ph∆∞∆°ng. Do ƒë√≥, du l·ªãch b·ªÅn v·ªØng ƒëang tr·ªü th√†nh xu h∆∞·ªõng, v·ªõi vi·ªác khuy·∫øn kh√≠ch du kh√°ch tham gia v√†o c√°c ho·∫°t ƒë·ªông b·∫£o v·ªá m√¥i tr∆∞·ªùng, t√¥n tr·ªçng vƒÉn h√≥a ƒë·ªãa ph∆∞∆°ng v√† h·ªó tr·ª£ kinh t·∫ø cho c·ªông ƒë·ªìng b·∫£n ƒë·ªãa.
        """,
        """
        **B√†i vi·∫øt 4: ·ª®ng d·ª•ng tr√≠ tu·ªá nh√¢n t·∫°o trong y t·∫ø Vi·ªát Nam**

        Tr√≠ tu·ªá nh√¢n t·∫°o (AI) ƒëang ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong lƒ©nh v·ª±c y t·∫ø t·∫°i Vi·ªát Nam. C√°c b·ªánh vi·ªán v√† trung t√¢m y t·∫ø ƒë√£ s·ª≠ d·ª•ng AI ƒë·ªÉ ch·∫©n ƒëo√°n h√¨nh ·∫£nh, d·ª± ƒëo√°n b·ªánh t·∫≠t v√† qu·∫£n l√Ω h·ªì s∆° b·ªánh √°n. V√≠ d·ª•, B·ªánh vi·ªán B·∫°ch Mai ƒë√£ tri·ªÉn khai h·ªá th·ªëng AI gi√∫p ch·∫©n ƒëo√°n s·ªõm b·ªánh ung th∆∞ ph·ªïi, c·∫£i thi·ªán hi·ªáu qu·∫£ ƒëi·ªÅu tr·ªã v√† gi·∫£m chi ph√≠ cho b·ªánh nh√¢n. Tuy nhi√™n, vi·ªác ƒë√†o t·∫°o nh√¢n l·ª±c v√† ƒë·∫£m b·∫£o an to√†n d·ªØ li·ªáu l√† nh·ªØng th√°ch th·ª©c c·∫ßn ƒë∆∞·ª£c gi·∫£i quy·∫øt.
        """,
        """
        **B√†i vi·∫øt 5: Ph√°t tri·ªÉn nƒÉng l∆∞·ª£ng t√°i t·∫°o ·ªü Vi·ªát Nam**

        Tr∆∞·ªõc nhu c·∫ßu nƒÉng l∆∞·ª£ng ng√†y c√†ng tƒÉng v√† √°p l·ª±c gi·∫£m ph√°t th·∫£i kh√≠ nh√† k√≠nh, Vi·ªát Nam ƒë√£ ƒë·∫ßu t∆∞ m·∫°nh m·∫Ω v√†o nƒÉng l∆∞·ª£ng t√°i t·∫°o. C√°c d·ª± √°n ƒëi·ªán m·∫∑t tr·ªùi v√† ƒëi·ªán gi√≥ ƒë√£ ƒë∆∞·ª£c tri·ªÉn khai t·∫°i nhi·ªÅu t·ªânh th√†nh, ƒë·∫∑c bi·ªát l√† ·ªü mi·ªÅn Trung v√† mi·ªÅn Nam. Ch√≠nh ph·ªß ƒë·∫∑t m·ª•c ti√™u ƒë·∫øn nƒÉm 2030, nƒÉng l∆∞·ª£ng t√°i t·∫°o s·∫Ω chi·∫øm 30% t·ªïng c√¥ng su·∫•t ƒëi·ªán qu·ªëc gia. Tuy nhi√™n, vi·ªác t√≠ch h·ª£p nƒÉng l∆∞·ª£ng t√°i t·∫°o v√†o l∆∞·ªõi ƒëi·ªán v√† ƒë·∫£m b·∫£o ·ªïn ƒë·ªãnh cung c·∫•p ƒëi·ªán l√† nh·ªØng th√°ch th·ª©c c·∫ßn ƒë∆∞·ª£c quan t√¢m.
        """,
        """
        **B√†i vi·∫øt 6: Th·ª±c tr·∫°ng v√† gi·∫£i ph√°p cho giao th√¥ng ƒë√¥ th·ªã t·∫°i H√† N·ªôi**

        H√† N·ªôi, th·ªß ƒë√¥ c·ªßa Vi·ªát Nam, ƒëang ƒë·ªëi m·∫∑t v·ªõi v·∫•n ƒë·ªÅ √πn t·∫Øc giao th√¥ng nghi√™m tr·ªçng. S·ª± gia tƒÉng nhanh ch√≥ng c·ªßa s·ªë l∆∞·ª£ng xe c√° nh√¢n, h·∫° t·∫ßng giao th√¥ng ch∆∞a ƒë√°p ·ª©ng k·ªãp v√† √Ω th·ª©c tham gia giao th√¥ng c·ªßa ng∆∞·ªùi d√¢n c√≤n h·∫°n ch·∫ø l√† nh·ªØng nguy√™n nh√¢n ch√≠nh. ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, th√†nh ph·ªë ƒë√£ tri·ªÉn khai nhi·ªÅu gi·∫£i ph√°p nh∆∞ ph√°t tri·ªÉn h·ªá th·ªëng giao th√¥ng c√¥ng c·ªông, x√¢y d·ª±ng c√°c tuy·∫øn ƒë∆∞·ªùng v√†nh ƒëai v√† √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin trong qu·∫£n l√Ω giao th√¥ng.
        """,
        """
        **B√†i vi·∫øt 7: Vai tr√≤ c·ªßa ph·ª• n·ªØ trong kinh t·∫ø Vi·ªát Nam hi·ªán ƒë·∫°i**

        Ph·ª• n·ªØ Vi·ªát Nam ng√†y c√†ng kh·∫≥ng ƒë·ªãnh vai tr√≤ quan tr·ªçng trong n·ªÅn kinh t·∫ø. H·ªç kh√¥ng ch·ªâ tham gia v√†o l·ª±c l∆∞·ª£ng lao ƒë·ªông m√† c√≤n gi·ªØ nhi·ªÅu v·ªã tr√≠ l√£nh ƒë·∫°o trong c√°c doanh nghi·ªáp v√† t·ªï ch·ª©c. C√°c ch∆∞∆°ng tr√¨nh h·ªó tr·ª£ kh·ªüi nghi·ªáp cho ph·ª• n·ªØ ƒë√£ gi√∫p nhi·ªÅu doanh nh√¢n n·ªØ th√†nh c√¥ng. Tuy nhi√™n, ph·ª• n·ªØ v·∫´n ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu th√°ch th·ª©c nh∆∞ ch√™nh l·ªách thu nh·∫≠p, ƒë·ªãnh ki·∫øn gi·ªõi v√† tr√°ch nhi·ªám gia ƒë√¨nh.
        """,
        """
        **B√†i vi·∫øt 8: ·∫¢nh h∆∞·ªüng c·ªßa m·∫°ng x√£ h·ªôi ƒë·∫øn gi·ªõi tr·∫ª Vi·ªát Nam**

        M·∫°ng x√£ h·ªôi ƒë√£ tr·ªü th√†nh m·ªôt ph·∫ßn kh√¥ng th·ªÉ thi·∫øu trong cu·ªôc s·ªëng c·ªßa gi·ªõi tr·∫ª Vi·ªát Nam. N√≥ mang l·∫°i nhi·ªÅu l·ª£i √≠ch nh∆∞ k·∫øt n·ªëi, chia s·∫ª th√¥ng tin v√† gi·∫£i tr√≠. Tuy nhi√™n, vi·ªác s·ª≠ d·ª•ng m·∫°ng x√£ h·ªôi qu√° m·ª©c c≈©ng g√¢y ra nhi·ªÅu v·∫•n ƒë·ªÅ nh∆∞ nghi·ªán internet, gi·∫£m t∆∞∆°ng t√°c x√£ h·ªôi th·ª±c t·∫ø v√† ·∫£nh h∆∞·ªüng ƒë·∫øn s·ª©c kh·ªèe t√¢m l√Ω. Do ƒë√≥, c·∫ßn c√≥ s·ª± h∆∞·ªõng d·∫´n v√† gi√°o d·ª•c ƒë·ªÉ gi·ªõi tr·∫ª s·ª≠ d·ª•ng m·∫°ng x√£ h·ªôi m·ªôt c√°ch l√†nh m·∫°nh v√† hi·ªáu qu·∫£.
        """,
        """
        **B√†i vi·∫øt 9: B·∫£o t·ªìn vƒÉn h√≥a truy·ªÅn th·ªëng trong th·ªùi k·ª≥ h·ªôi nh·∫≠p**

        Trong b·ªëi c·∫£nh h·ªôi nh·∫≠p qu·ªëc t·∫ø, vi·ªác b·∫£o t·ªìn v√† ph√°t huy vƒÉn h√≥a truy·ªÅn th·ªëng l√† m·ªôt th√°ch th·ª©c l·ªõn ƒë·ªëi v·ªõi Vi·ªát Nam. Nhi·ªÅu gi√° tr·ªã vƒÉn h√≥a ƒëang d·∫ßn b·ªã mai m·ªôt do ·∫£nh h∆∞·ªüng c·ªßa vƒÉn h√≥a ngo·∫°i lai v√† s·ª± thay ƒë·ªïi c·ªßa x√£ h·ªôi. C√°c ch∆∞∆°ng tr√¨nh gi√°o d·ª•c, l·ªÖ h·ªôi truy·ªÅn th·ªëng v√† ho·∫°t ƒë·ªông c·ªông ƒë·ªìng ƒë√£ ƒë∆∞·ª£c t·ªï ch·ª©c nh·∫±m gi·ªØ g√¨n v√† truy·ªÅn b√° vƒÉn h√≥a d√¢n t·ªôc cho th·∫ø h·ªá tr·∫ª.
        """,
        """
        **B√†i vi·∫øt 10: T√°c ƒë·ªông c·ªßa bi·∫øn ƒë·ªïi kh√≠ h·∫≠u ƒë·∫øn n√¥ng nghi·ªáp Vi·ªát Nam**

        Bi·∫øn ƒë·ªïi kh√≠ h·∫≠u ƒëang ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn n√¥ng nghi·ªáp Vi·ªát Nam. Hi·ªán t∆∞·ª£ng th·ªùi ti·∫øt c·ª±c ƒëoan, m·ª±c n∆∞·ªõc bi·ªÉn d√¢ng v√† s·ª± thay ƒë·ªïi c·ªßa m√πa v·ª• ƒë√£ g√¢y ra nhi·ªÅu kh√≥ khƒÉn cho n√¥ng d√¢n. ƒê·ªÉ th√≠ch ·ª©ng, nhi·ªÅu bi·ªán ph√°p ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng nh∆∞ chuy·ªÉn ƒë·ªïi c∆° c·∫•u c√¢y tr·ªìng, √°p d·ª•ng c√¥ng ngh·ªá n√¥ng nghi·ªáp th√¥ng minh v√† x√¢y d·ª±ng h·ªá th·ªëng th·ªßy l·ª£i b·ªÅn v·ªØng. Tuy nhi√™n, c·∫ßn c√≥ s·ª± h·ªó tr·ª£ t·ª´ ch√≠nh ph·ªß v√† c·ªông ƒë·ªìng qu·ªëc t·∫ø ƒë·ªÉ ƒë·∫£m b·∫£o an ninh l∆∞∆°ng th·ª±c v√† sinh k·∫ø cho ng∆∞·ªùi d√¢n.
        """
        ]
        result = self.process_articles_in_batches(articles, batch_size=5)
        print(result)
        # print(type(result)) # a list of list
        # for i, tags in enumerate(result):
        #     print(f"Article {i+1} Tags: {', '.join(tags)}")
        return
